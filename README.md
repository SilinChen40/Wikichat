# Wikichat

## Motivation - The Rise of Conversational AI and the Factuality Challenge

* **Widespread Use of LLMs**: Large Language Models (LLMs) like GPT-4 have become essential tools for chatbots, enabling natural and engaging conversations.

* **The Problem with Hallucination**: Despite their conversational strengths, LLMs often produce factually incorrect information, or "hallucinate," particularly on lesser-known topics or recent events.

* **Demand for Reliable AI**: As more applications adopt LLMs for real-world use cases (healthcare, education, knowledge-sharing), there is an urgent need for chatbots that provide accurate, up-to-date, and reliable information.

<p align="center" width="100%">
<img src="images\llm_hall.png" alt="" style="width: 60%; min-width: 300px; display: block; margin: auto;">
</p>

## Problem Statement - Addressing Hallucination in LLMs: Accuracy vs. Engagement 

* **Factual Inaccuracy**: LLMs generate responses that are not always grounded in verified information, leading to potential misinformation.

* **Challenges with Tail and Recent Topics**:

  * Tail Topics: Niche subjects are often less represented in pre-trained data, increasing the likelihood of hallucination.

  * Recent Knowledge: LLMs cannot access events that occurred after their training cut-off, making them unreliable for current information.

* **Balancing Accuracy and Conversationality**: Efforts to ensure factual accuracy often come at the cost of conversational quality, making responses less engaging or overly simplified.

## Approach

1. **Retrieve**: The user query initiates a search in Wikipedia to gather relevant, up-to-date information.

2. **Generate**: An LLM generates a preliminary response based on the retrieved information.

3. **Extract and Verify Claims**: Each claim from the response is fact-checked using evidence from Wikipedia to filter out unverified statements.

4. **Draft and Refine**: The system drafts an initial response with only verified claims, then refines it to improve clarity, naturalness, and engagement.

## Key Questions

1. How does WikiChat ensure that the claims made by the chatbot are factually accurate, and what methods does it use to verify each claim?
2. What challenges does WikiChat face when dealing with recent or less popular (tail) topics, and how does the pipeline address these challenges?

## Architecture Overview

```python
# WikiChat 7-Stage Pipeline for Factual Chatbot Responses

function WikiChat_Response(user_query):
    # Stage 1: Retrieve relevant information
    query = generate_query(user_query)  # generate query based on user input
    retrieved_passages = retrieve_from_wikipedia(query)  # fetch passages from Wikipedia

    # Stage 2: Summarize and filter the retrieved information
    summary_bullets = summarize_and_filter(retrieved_passages)  # filter irrelevant data, summarize important info

    # Stage 3: Generate initial response with LLM
    initial_response = LLM_generate_response(summary_bullets, user_query)  # use LLM to create a response

    # Stage 4: Extract claims from the generated response
    claims = extract_claims(initial_response)  # split response into individual claims

    # Stage 5: Fact-check each claim
    verified_claims = []
    for claim in claims:
        evidence = retrieve_evidence(claim)  # retrieve relevant evidence for the claim
        if verify_claim(claim, evidence):
            verified_claims.append(claim)  # add to list if verified as factual

    # Stage 6: Draft response with verified claims
    draft_response = draft_with_verified_claims(verified_claims)  # compile verified claims into a draft response

    # Stage 7: Refine response for conversational quality
    final_response = refine_response(draft_response, user_query)  # enhance naturalness, relevance, and clarity

    return final_response

# Main Function Execution
output = WikiChat_Response(user_query)

```

### Understanding WikiChat’s Unique Approach

* **Retrieve-then-Generate Approach**: Unlike previous models that either rely entirely on retrieval (giving dry, fact-based responses) or generation (risking hallucinations), WikiChat combines both methods. It retrieves accurate information and then generates responses, ensuring they are engaging yet factual.

* **Fact-Checking Pipeline**: Traditional LLM chatbots lack a structured verification process, leading to hallucinations. WikiChat’s approach includes a claim extraction and verification process, where each claim generated by the LLM is cross-checked with Wikipedia data. This makes WikiChat unique in its rigorous filtering of potentially inaccurate information.

* **Refinement Stage for Conversational Quality**: In contrast to fact-checking systems that sacrifice engagement for accuracy, WikiChat has a final refinement stage. This step is designed to improve the response's conversational flow, making it natural and user-friendly.

<p align="center" width="100%">
<img src="images\wikipipeline.png" alt="" style="width: 60%; min-width: 300px; display: block; margin: auto;">
</p>

## Critical Analysis: Key Limitations and Opportunities for Improvemen

* **Reliance on Wikipedia as a Primary Knowledge Source**: While Wikipedia is a widely trusted and frequently updated source, its coverage may be insufficient for specialized or emerging topics, such as certain areas in medicine or law. This reliance could limit WikiChat’s effectiveness in domains where comprehensive, specialized knowledge is essential. Expanding WikiChat’s capacity to draw from other reliable databases could make it more versatile and adaptable.

* **Latency Challenges Due to the 7-Stage Pipeline**: WikiChat’s multi-step approach, involving claim extraction, verification, and refinement, could lead to slower response times, especially in high-demand, real-time scenarios. Although this design enhances factual accuracy, future work on optimizing or parallelizing the process could improve response speed without sacrificing reliability.



## Impact of WikiChat on the AI Landscape

* **Enhancing Factual Reliability in Conversational AI**: WikiChat addresses a major limitation of traditional LLMs: hallucination, or the generation of plausible but incorrect information. By implementing a fact-checking pipeline grounded in Wikipedia, WikiChat has set a new standard for factual accuracy in conversational AI. This emphasis on reliability is crucial as AI systems become more integrated into critical applications (e.g., education, healthcare).

* **Influencing Future Research and Development**: The WikiChat approach demonstrates how retrieval-based methods can be combined with generative models to improve accuracy without sacrificing conversational quality. This work is likely to inspire further research into hybrid models that prioritize grounding responses in verifiable sources. Additionally, WikiChat’s pipeline may encourage other researchers to develop specialized fact-checking systems, potentially using multi-source verification and domain-specific databases. This evolution could help shape the next generation of reliable AI assistants and knowledge-based systems

* **Broader Importance and Intersections**: WikiChat intersects with ongoing research on interpretability and trustworthiness in AI, aligning with the goals of making AI systems more accountable and understandable for users. This work also contributes to ethical AI development by aiming to reduce misinformation and increase transparency in AI-driven responses, aligning with long-term goals in responsible AI practices.

## Citation

Semnani, Shima Jazayeri, Victor Zhongkai Yao, Hongchang Zhang, and Monica S. Lam. **"WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia."** arXiv preprint arXiv:2305.14292 (2023).

## Resources

- [WikiChat GitHub Repository](https://github.com/stanford-oval/WikiChat): Access the code and implementation details for WikiChat.
- [WikiChat Paper on arXiv](https://arxiv.org/abs/2305.14292): Read the full research paper for an in-depth understanding of the approach and methodology.
















